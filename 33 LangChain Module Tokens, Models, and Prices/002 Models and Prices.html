<p>Welcome back, everyone!</p><p>In the last lesson, we discussed what tokens are in the context of OpenAI’s large language models. We noted that a token is roughly 3/4 of a word. As a rule of thumb—and an easier to remember—100 tokens amount to approximately 75 words.</p><p>Pay attention here, as OpenAI’s large language model prices are based on token consumption. The larger the text we give as an input or require as an output, the higher the price becomes. Of course, the price is also model-dependent, so let’s visit OpenAI’s Pricing page, study the models available, and discuss their price points.</p><p><a href="https://openai.com/api/pricing/" rel="noopener noreferrer" target="_blank"><strong>https://openai.com/api/pricing/</strong></a></p><p>First comes GPT-4o. This is OpenAI’s most efficient, affordable, and advanced model, with training data up to October 2023. It accepts information in text or images and outputs a text return.</p><p>Now, you see that the price is determined by the number of input <strong>and</strong> output tokens, with each type being priced differently. Intuitively, input tokens are passed to the model as a prompt. As of the time of writing, you’ll be charged 5.00 US dollars per million tokens. This is quite a low price considering a million tokens amount to approximately a 1,500-page novel.</p><p>In contrast, output tokens are returned by the language model as a response. They’re priced a bit higher, at 15.00 US dollars per million tokens. If you find it more convenient, you can display the same price per thousand tokens instead.</p><p>Let me insert an important remark here. We cannot prompt something as big as The Lord of the Rings novel into a language model. Nor should we expect it to return a response of the same length because each model has a certain context window limit. In the case of the GPT-4o model, we were limited to a context window as large as 128,000 tokens, shared between input and output tokens.</p><p>So, to sum up, there are a few points to consider when choosing the suitable language model for your project:</p><ul><li><p>the price per input and output tokens,</p></li><li><p>the cutoff date of the training data, and</p></li><li><p>the context window limit.</p></li></ul><p>In the practical part of this course, we’ll be using the GPT-4 model, but you’re welcome to insert any model you like. It’ll be as easy as changing a single string in the code.</p><p>Okay.</p><p>Next, we see the available embedding models, representing text as numerical vectors that capture its semantic meaning. (We'll discuss this topic in depth later in the course.) We’ll use the first embedding model on the list: text-embedding-3-small.</p><p>Towards the end of the page, we find OpenAI’s fine-tuning, image and audio models, which are all outside the scope of the course.</p><p>With that, we conclude our discussion on tokens, models, and prices. It’s now time we start practicing. But first things first: we need to set up our working environment. In the next section, I’ll guide you through creating a new Anaconda environment and obtaining an OpenAI API key.</p>