<p>Hey everyone!</p><p>In the previous section, we played around with the OpenAI APIâ€”examining message roles and key LLM parameters like temperature, token limit, and the streaming feature. Now that we have the foundations for creating chatbots, itâ€™s time to utilize the OpenAI API through LangChain.</p><p>But before we start coding, letâ€™s conceptually discuss the LangChain framework. As mentioned in previous lessons, LangChain allows the creation of stateful, context-aware, and reasoning LLM-powered applications. To make this quite difficult task possible, LangChainâ€™s developers have divided it into several integration components, each responsible for individual sub-tasks.</p><p>The first two components of an LLM-powered application are the model input and model output, falling under the umbrella term Model I/O. The module includes a large language model, a prompt, an example selector, and an output parser. Let me include prompt templates here and begin our discussion with prompts and prompt templates.</p><p>As you know, prompts are the questions and instructions we pass to a model in text format. For example:<br><em>Iâ€™ve recently adopted a dog. Could you suggest some dog names?</em></p><p>Prompt templates, in turn, provide an abstraction to prompts, making them reusable. Continuing the same example, imagine the word â€˜dogâ€™ becomes a placeholder for any pet, like so:<br><em>Iâ€™ve recently adopted a {pet}. Could you suggest some {pet} names?</em></p><p>We can then pass the word â€˜dogâ€™ as an input, and the model will return a list of possible dog names.</p><p>Such prompt abstractions are especially handy whenever we want to provide a model with examples to learn from â€“ a technique called few-shot prompting. (But more on this in upcoming lessons!) ðŸ˜Š So, thatâ€™s how we deal with model inputs.</p><p>What about model outputs?</p><p>As youâ€™ve seen, the output from a model comes in the form of an assistant-role message. However, we often need the output in other formats. This is where output parsers come into play. In a section dedicated to output parsers, weâ€™ll discuss ones that parse outputs into strings, datetime objects, or comma-separated lists.</p><p>So, the Model I/O module comprises a prompt, a model, and an output parser.</p><p>Next up is the Retrieval module. As weâ€™ve previously discussed, retrievers are components we can add to our chatbots to make them context-awareâ€”that is, we can strategically feed them external or proprietary information so that they can answer questions on data they havenâ€™t trained on. Several components make this possible:</p><ul><li><p>document loader,</p></li><li><p>text splitter,</p></li><li><p>embedding model,</p></li><li><p>vector store,</p></li><li><p>and retriever.</p></li></ul><p>This is the order in which weâ€™ll discuss them in the course. Ultimately, weâ€™ll create a chatbot to answer questions on 365â€™s Introduction to Data and Data Science course.</p><p>The third and final module is Agent Tooling, which allows reasoning chatbots to be created. These chatbots can choose among different tools and their order of execution to solve various tasks. Tools for specific tasks can be combined to form toolkits. Although our course wonâ€™t cover toolkits, itâ€™s beneficial to know that you can create them or use ones integrated into LangChain.</p><p>Another component of the LangChain framework is the specific protocolâ€”known as LangChain Expression Languageâ€”that links the three modules. It forms a significant portion of our course. This subject is best learned by doing; therefore, I will not cover it further in this lesson.</p><p>One last part of the framework weâ€™ll demonstrate in the course is the templates, which are pre-implemented prompt templates that have proven helpful for specific tasks.</p><p>And with that, we exhaust the modules covered in the course.</p><p>Whatâ€™s left are the LangSmith platform and the LangServe libraries, which we briefly mentioned earlier. The former is responsible for a productâ€™s observability, while the latter is for the deployment.</p><p>Regarding LanagChain's documentation, allow me to briefly address this topic. Two main web pages will tremendously help you in the learning process. The first is <a href="https://python.langchain.com/v0.2/docs/introduction/" rel="noopener noreferrer" target="_blank">LangChainâ€™s documentation page</a>, where you can find plenty of tutorials, how-to guides, a conceptual guide, and a list of integrations. Itâ€™s a valuable resource that youâ€™re encouraged to read and re-read. ðŸ˜Š</p><p>The second is the libraryâ€™s <a href="https://api.python.langchain.com/en/latest/langchain_api_reference.html" rel="noopener noreferrer" target="_blank">API reference</a>. This essential aspect of programming documents every function, class, and method. (Weâ€™ll refer to this resource many times throughout our journey.)</p><p>Alright, I think weâ€™re now good to go!</p>